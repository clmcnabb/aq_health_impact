{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH = Path(\"data\", \"input\", \"air_quality_health_impact_data.csv\")\n",
    "air = pd.read_csv(DATA_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "This section is dedicated to understanding the data, identifying and correcting missing values, and examing distributions of features. Additionally, we will look at the covariance and correlation matrices to see if, visually, we can identify any basic trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Observations: {air.shape[0]}\")\n",
    "print(f\"Features: {air.shape[1]}\")\n",
    "air.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 20\n",
    "\n",
    "fig, axs = plt.subplots(7, 2, figsize=(12, 16))\n",
    "\n",
    "axs[0, 0].hist(air[\"AQI\"], bins=bins)\n",
    "axs[0, 0].set_ylabel(\"Count\")\n",
    "axs[0, 0].set_title(\"Distribution of AQI\")\n",
    "\n",
    "axs[0, 1].hist(air[\"PM10\"], bins=bins)\n",
    "axs[0, 1].set_ylabel(\"Count\")\n",
    "axs[0, 1].set_title(\"Distribution of PM10\")\n",
    "\n",
    "axs[1, 0].hist(air[\"PM2_5\"], bins=bins)\n",
    "axs[1, 0].set_ylabel(\"Count\")\n",
    "axs[1, 0].set_title(\"Distribution of PM2_5\")\n",
    "\n",
    "axs[1, 1].hist(air[\"NO2\"], bins=bins)\n",
    "axs[1, 1].set_ylabel(\"Count\")\n",
    "axs[1, 1].set_title(\"Distribution of NO2\")\n",
    "\n",
    "axs[2, 0].hist(air[\"SO2\"], bins=bins)\n",
    "axs[2, 0].set_ylabel(\"Count\")\n",
    "axs[2, 0].set_title(\"Distribution of SO2\")\n",
    "\n",
    "axs[2, 1].hist(air[\"O3\"], bins=bins)\n",
    "axs[2, 1].set_ylabel(\"Count\")\n",
    "axs[2, 1].set_title(\"Distribution of O3\")\n",
    "\n",
    "axs[3, 0].hist(air[\"Temperature\"], bins=bins)\n",
    "axs[3, 0].set_ylabel(\"Count\")\n",
    "axs[3, 0].set_title(\"Distribution of Temperature\")\n",
    "\n",
    "axs[3, 1].hist(air[\"Humidity\"], bins=bins)\n",
    "axs[3, 1].set_ylabel(\"Count\")\n",
    "axs[3, 1].set_title(\"Distribution of Humidity\")\n",
    "\n",
    "axs[4, 0].hist(air[\"WindSpeed\"], bins=bins)\n",
    "axs[4, 0].set_ylabel(\"Count\")\n",
    "axs[4, 0].set_title(\"Distribution of WindSpeed\")\n",
    "\n",
    "axs[4, 1].hist(air[\"RespiratoryCases\"], bins=bins)\n",
    "axs[4, 1].set_ylabel(\"Count\")\n",
    "axs[4, 1].set_title(\"Distribution of RespiratoryCases\")\n",
    "\n",
    "axs[5, 0].hist(air[\"CardiovascularCases\"], bins=bins)\n",
    "axs[5, 0].set_ylabel(\"Count\")\n",
    "axs[5, 0].set_title(\"Distribution of CardiovascularCases\")\n",
    "\n",
    "axs[5, 1].hist(air[\"HospitalAdmissions\"], bins=bins)\n",
    "axs[5, 1].set_ylabel(\"Count\")\n",
    "axs[5, 1].set_title(\"Distribution of HospitalAdmissions\")\n",
    "\n",
    "axs[6, 0].hist(air[\"HealthImpactScore\"], bins=bins)\n",
    "axs[6, 0].set_ylabel(\"Count\")\n",
    "axs[6, 0].set_title(\"Distribution of HealthImpactScore\")\n",
    "\n",
    "axs[6, 1].hist(air[\"HealthImpactClass\"], bins=bins)\n",
    "axs[6, 1].set_ylabel(\"Count\")\n",
    "axs[6, 1].set_title(\"Distribution of HealthImpactClass\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the covariance matrix of the dataset. The \"covariance\" measures how changes in one variable are associated with changes in a second variable. In other words, the covariance measures the degree to which two variables are linearly associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air.cov(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few examples of what we can find from the data here include:\n",
    "\n",
    "1. The high covariance (286.94) of AQI and PM10 indicates that a higher AQI is associated with a higher level of particulates 10 Âµm or smaller.\n",
    "2. The extremely high covariance (1185.04) between AQI and HealthImpactScore indicates that higher AQI is associated with a higher Health Impact Score.\n",
    "3. The negative covariance between O3 and NO2 (-74.46) indicates that higher Ozone is associated with lower levels of nitrogen dioxide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now display the correlation matrix. \"Correlation\" measures both the strength and direction of the linear relationship between two variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will define and utilize functions to split the data into training, validation, and test splits. Additionally, we will create preprocessing steps to scale the data by removing the mean and dividing by the standard deviation. This returns us a feature centered around 0, making learning easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import tarfile\n",
    "import tempfile\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "\n",
    "\n",
    "def preprocess(base_directory, job_type=\"classification\"):\n",
    "    \"\"\"Load the supplied data, split it and transform it.\"\"\"\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    if job_type == \"classification\":\n",
    "        target_transformer = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"Class\", FunctionTransformer(np.abs), [\"HealthImpactClass\"])\n",
    "            ],\n",
    "        )\n",
    "    else:\n",
    "        target_transformer = ColumnTransformer(\n",
    "            transformers=[(\"Score\", StandardScaler(), [\"HealthImpactScore\"])],\n",
    "        )\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        StandardScaler(),\n",
    "    )\n",
    "\n",
    "    features_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"numeric\",\n",
    "                numeric_transformer,\n",
    "                make_column_selector(dtype_exclude=\"object\"),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df_train, df_validation, df_test = _split_data(df)\n",
    "\n",
    "    y_train = target_transformer.fit_transform(df_train)\n",
    "    y_validation = target_transformer.transform(df_validation)\n",
    "    y_test = target_transformer.transform(df_test)\n",
    "\n",
    "    _save_train_baseline(base_directory, df_train)\n",
    "    _save_test_baseline(base_directory, df_test)\n",
    "\n",
    "    df_train = df_train.drop(\"HealthImpactClass\", axis=1)\n",
    "    df_validation = df_validation.drop(\"HealthImpactClass\", axis=1)\n",
    "    df_test = df_test.drop(\"HealthImpactClass\", axis=1)\n",
    "\n",
    "    df_train = df_train.drop(\"HealthImpactScore\", axis=1)\n",
    "    df_validation = df_validation.drop(\"HealthImpactScore\", axis=1)\n",
    "    df_test = df_test.drop(\"HealthImpactScore\", axis=1)\n",
    "\n",
    "    X_train = features_transformer.fit_transform(df_train)  # noqa: N806\n",
    "    X_validation = features_transformer.transform(df_validation)  # noqa: N806\n",
    "    X_test = features_transformer.transform(df_test)  # noqa: N806\n",
    "\n",
    "    _save_splits(\n",
    "        base_directory,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_validation,\n",
    "        y_validation,\n",
    "        X_test,\n",
    "        y_test,\n",
    "    )\n",
    "    _save_model(base_directory, target_transformer, features_transformer)\n",
    "\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads every CSV file available and\n",
    "    concatenates them into a single dataframe.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _split_data(df):\n",
    "    \"\"\"Split the data into train, validation, and test.\"\"\"\n",
    "    df_train, temp = train_test_split(df, test_size=0.3)\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "\n",
    "def _save_train_baseline(base_directory, df_train):\n",
    "    \"\"\"Save the untransformed training data to disk.\n",
    "\n",
    "    We will need the training data to compute a baseline to\n",
    "    determine the quality of the data that the model receives\n",
    "    when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"train-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_train.copy().dropna()\n",
    "\n",
    "    # To compute the data quality baseline, we don't need the\n",
    "    # target variable, so we'll drop it from the dataframe.\n",
    "    df = df.drop(\"HealthImpactClass\", axis=1)\n",
    "    df = df.drop(\"HealthImpactScore\", axis=1)\n",
    "\n",
    "    df.to_csv(baseline_path / \"train-baseline.csv\", header=True, index=False)\n",
    "\n",
    "\n",
    "def _save_test_baseline(base_directory, df_test):\n",
    "    \"\"\"Save the untransformed test data to disk.\n",
    "\n",
    "    We will need the test data to compute a baseline to\n",
    "    determine the quality of the model predictions when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"test-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_test.copy().dropna()\n",
    "\n",
    "    # We'll use the test baseline to generate predictions later,\n",
    "    # and we can't have a header line because the model won't be\n",
    "    # able to make a prediction for it.\n",
    "    df.to_csv(baseline_path / \"test-baseline.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory,\n",
    "    X_train,  # noqa: N803\n",
    "    y_train,\n",
    "    X_validation,  # noqa: N803\n",
    "    y_validation,\n",
    "    X_test,  # noqa: N803\n",
    "    y_test,\n",
    "):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    validation = np.concatenate((X_validation, y_validation), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        validation_path / \"validation.csv\",\n",
    "        header=False,\n",
    "        index=False,\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def _save_model(base_directory, target_transformer, features_transformer):\n",
    "    \"\"\"Save the Scikit-Learn transformation pipelines.\n",
    "\n",
    "    This function creates a model.tar.gz file that\n",
    "    contains the two transformation pipelines we built\n",
    "    to transform the data.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as directory:\n",
    "        joblib.dump(target_transformer, Path(directory) / \"target.joblib\")\n",
    "        joblib.dump(features_transformer, Path(directory) / \"features.joblib\")\n",
    "\n",
    "        model_path = Path(base_directory) / \"model\"\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with tarfile.open(f\"{(model_path / 'model.tar.gz').as_posix()}\", \"w:gz\") as tar:\n",
    "            tar.add(Path(directory) / \"target.joblib\", arcname=\"target.joblib\")\n",
    "            tar.add(\n",
    "                Path(directory) / \"features.joblib\",\n",
    "                arcname=\"features.joblib\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = Path(\"data\")\n",
    "preprocess(base_directory, job_type=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will define and test the model using the sklearn library. Different models will be used, and different hyperparameters will be tested in order to try to find the best configuration for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "This first section will examine XGBoost, a popular machine learning framework for regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "train_path = Path(base_directory, \"train\")\n",
    "X_train = pd.read_csv(Path(train_path, \"train.csv\"), header=None)\n",
    "y_train = X_train[X_train.columns[-1]]\n",
    "X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "\n",
    "val_path = Path(base_directory, \"validation\")\n",
    "X_val = pd.read_csv(Path(val_path, \"validation.csv\"), header=None)\n",
    "y_val = X_val[X_val.columns[-1]]\n",
    "X_val = X_val.drop(X_val.columns[-1], axis=1)\n",
    "\n",
    "# Define the hyperparameters and their search ranges\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Create an XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring=\"accuracy\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring=\"accuracy\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the model with the best hyperparameters on the entire dataset\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "accuracy = best_model.score(X_val, y_val)\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
